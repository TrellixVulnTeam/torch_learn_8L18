{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80069e1d-66cf-41d2-997c-1a74de437a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:17:55.950694Z",
     "iopub.status.busy": "2022-05-28T13:17:55.950072Z",
     "iopub.status.idle": "2022-05-28T13:17:55.961328Z",
     "shell.execute_reply": "2022-05-28T13:17:55.960373Z",
     "shell.execute_reply.started": "2022-05-28T13:17:55.950642Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入各种包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import ttach as tta\n",
    "from resnest.torch import resnest50\n",
    "\n",
    "from cutmix.cutmix import CutMix\n",
    "from cutmix.utils import CutMixCrossEntropyLoss\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "# This is for the progress bar.\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5389d56b-783e-4f3d-8dba-54b33a8d2372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:51.403505Z",
     "iopub.status.busy": "2022-05-28T13:05:51.403155Z",
     "iopub.status.idle": "2022-05-28T13:05:51.431543Z",
     "shell.execute_reply": "2022-05-28T13:05:51.431049Z",
     "shell.execute_reply.started": "2022-05-28T13:05:51.403475Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/0.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/1.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/2.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/3.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/4.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image             label\n",
       "0  images/0.jpg  maclura_pomifera\n",
       "1  images/1.jpg  maclura_pomifera\n",
       "2  images/2.jpg  maclura_pomifera\n",
       "3  images/3.jpg  maclura_pomifera\n",
       "4  images/4.jpg  maclura_pomifera"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看label文件长啥样\n",
    "labels_dataframe = pd.read_csv('./dataset/classify_leaves/train.csv')\n",
    "labels_dataframe.head(5)\n",
    "# labels_test=pd.read_csv('./dataset/classify_leaves/test.csv')\n",
    "# labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbf36f31-0a61-4b08-bfd8-1d26c0c46de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:53.457214Z",
     "iopub.status.busy": "2022-05-28T13:05:53.456405Z",
     "iopub.status.idle": "2022-05-28T13:05:53.468560Z",
     "shell.execute_reply": "2022-05-28T13:05:53.467808Z",
     "shell.execute_reply.started": "2022-05-28T13:05:53.457160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把label文件排个序\n",
    "leaves_labels = sorted(list(set(labels_dataframe['label'])))\n",
    "n_classes = len(leaves_labels)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea9ade33-4dc4-482e-bc24-e39f2db58a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:54.370592Z",
     "iopub.status.busy": "2022-05-28T13:05:54.369989Z",
     "iopub.status.idle": "2022-05-28T13:05:54.376999Z",
     "shell.execute_reply": "2022-05-28T13:05:54.375994Z",
     "shell.execute_reply.started": "2022-05-28T13:05:54.370539Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cls_to_num = dict(zip(leaves_labels, range(len(leaves_labels))))\n",
    "num_to_cls = dict(zip(range(len(leaves_labels)), leaves_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12f52c14-3e7d-436a-904f-bc0a48918238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:56.265239Z",
     "iopub.status.busy": "2022-05-28T13:05:56.264644Z",
     "iopub.status.idle": "2022-05-28T13:05:56.281796Z",
     "shell.execute_reply": "2022-05-28T13:05:56.280865Z",
     "shell.execute_reply.started": "2022-05-28T13:05:56.265185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, csv_path, img_path, transform,resize_h=224, resize_w=224):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): csv 文件路径\n",
    "            img_path (string): 图像文件所在路径\n",
    "            mode (string): 训练模式还是测试模式\n",
    "            valid_ratio (float): 验证集比例\n",
    "        \"\"\"\n",
    "        self.resize_h, self.resize_w = resize_h, resize_w\n",
    "        self.img_path = img_path\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        self.data_len = len(self.data_info)\n",
    "        self.train_img = np.asarray(self.data_info.iloc[1:self.data_len, 0])\n",
    "        self.train_label = np.asarray(self.data_info.iloc[1:self.data_len, 1])\n",
    "        self.transform=transform\n",
    "        self.img_arr = self.train_img\n",
    "        self.label_arr = self.train_label\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_arr[index]\n",
    "        img_as_img = Image.open(os.path.join(self.img_path, img_name))\n",
    "        # 如果需要将RGB三通道的图片转换成灰度图片可参考下面两行\n",
    "        #         if img_as_img.mode != 'L':\n",
    "        #             img_as_img = img_as_img.convert('L')\n",
    "        # transform = transforms.Compose([\n",
    "        #     # transforms.RandomResizedCrop((224,224),scale=(0.8,1.0),ratio=(0.8,1.2)),#随机裁剪\n",
    "        #     # transforms.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),#随机调整图片的亮度、对比度、饱和度、色调\n",
    "        #     transforms.Resize((224,224)),#缩放图片\n",
    "        #     transforms.ToTensor(), ]  # 将图片转换成Tensor\n",
    "        # )\n",
    "        img_as_tensor = self.transform(img_as_img)\n",
    "        label = self.label_arr[index]\n",
    "        num_label = cls_to_num[label]\n",
    "        return img_as_tensor, num_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fcb8be3-95e7-4f28-8a47-6e871485d1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:56.645259Z",
     "iopub.status.busy": "2022-05-28T13:05:56.644674Z",
     "iopub.status.idle": "2022-05-28T13:05:56.658461Z",
     "shell.execute_reply": "2022-05-28T13:05:56.657621Z",
     "shell.execute_reply.started": "2022-05-28T13:05:56.645205Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    def __init__(self, csv_path, img_path, transform,resize_h=224, resize_w=224):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): csv 文件路径\n",
    "            img_path (string): 图像文件所在路径\n",
    "            mode (string): 训练模式还是测试模式\n",
    "            valid_ratio (float): 验证集比例\n",
    "        \"\"\"\n",
    "        self.resize_h, self.resize_w = resize_h, resize_w\n",
    "        self.img_path = img_path\n",
    "        self.transform=transform\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        self.data_len = len(self.data_info)\n",
    "        self.test_img = np.asarray(self.data_info.iloc[1:self.data_len, 0])\n",
    "        self.img_arr = self.test_img\n",
    "       \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_arr[index]\n",
    "        img_as_img = Image.open(os.path.join(self.img_path, img_name))\n",
    "        # 如果需要将RGB三通道的图片转换成灰度图片可参考下面两行\n",
    "        #         if img_as_img.mode != 'L':\n",
    "        #             img_as_img = img_as_img.convert('L')\n",
    "        # transform = transforms.Compose([\n",
    "        #     # transforms.RandomResizedCrop((224,224),scale=(0.8,1.0),ratio=(0.8,1.2)),#随机裁剪\n",
    "        #     # transforms.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),#随机调整图片的亮度、对比度、饱和度、色调\n",
    "        #     transforms.Resize((224,224)),#缩放图片\n",
    "        #     transforms.ToTensor()]  # 将图片转换成Tensor\n",
    "        # )\n",
    "        img_as_tensor = self.transform(img_as_img)\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b30653-ad0d-46c8-810c-807ba95476e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:57.597882Z",
     "iopub.status.busy": "2022-05-28T13:05:57.597068Z",
     "iopub.status.idle": "2022-05-28T13:05:57.608914Z",
     "shell.execute_reply": "2022-05-28T13:05:57.608067Z",
     "shell.execute_reply.started": "2022-05-28T13:05:57.597802Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform=transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224,scale=(0.08, 1.0),ratio=(3/4,4/3)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "                                   ])\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28e5e688-f8cf-425d-a819-9a3e560751d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:05:59.466155Z",
     "iopub.status.busy": "2022-05-28T13:05:59.465230Z",
     "iopub.status.idle": "2022-05-28T13:05:59.532746Z",
     "shell.execute_reply": "2022-05-28T13:05:59.532107Z",
     "shell.execute_reply.started": "2022-05-28T13:05:59.466102Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/0.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/1.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/2.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/3.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/4.jpg</td>\n",
       "      <td>maclura_pomifera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18348</th>\n",
       "      <td>images/18348.jpg</td>\n",
       "      <td>aesculus_glabra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18349</th>\n",
       "      <td>images/18349.jpg</td>\n",
       "      <td>liquidambar_styraciflua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18350</th>\n",
       "      <td>images/18350.jpg</td>\n",
       "      <td>cedrus_libani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18351</th>\n",
       "      <td>images/18351.jpg</td>\n",
       "      <td>prunus_pensylvanica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18352</th>\n",
       "      <td>images/18352.jpg</td>\n",
       "      <td>quercus_montana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18353 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image                    label\n",
       "0          images/0.jpg         maclura_pomifera\n",
       "1          images/1.jpg         maclura_pomifera\n",
       "2          images/2.jpg         maclura_pomifera\n",
       "3          images/3.jpg         maclura_pomifera\n",
       "4          images/4.jpg         maclura_pomifera\n",
       "...                 ...                      ...\n",
       "18348  images/18348.jpg          aesculus_glabra\n",
       "18349  images/18349.jpg  liquidambar_styraciflua\n",
       "18350  images/18350.jpg            cedrus_libani\n",
       "18351  images/18351.jpg      prunus_pensylvanica\n",
       "18352  images/18352.jpg          quercus_montana\n",
       "\n",
       "[18353 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'dataset/classify_leaves/train.csv'\n",
    "test_path = 'dataset/classify_leaves/test.csv'\n",
    "img_path = 'dataset/classify_leaves/'\n",
    "train_set=TrainData(train_path,img_path,train_transform)\n",
    "test_set=TestData(test_path,img_path,test_transform)\n",
    "pd.read_csv(train_path)\n",
    "# print(train_set.data_info)\n",
    "# print(test_set.data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08051a3-599f-41e6-893f-24fbd938cc34",
   "metadata": {},
   "source": [
    "### ResNeSt模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f14aba-fba6-4246-9d74-a13f9817be24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:27:59.554865Z",
     "iopub.status.busy": "2022-05-27T20:27:59.554275Z",
     "iopub.status.idle": "2022-05-27T20:27:59.564161Z",
     "shell.execute_reply": "2022-05-27T20:27:59.563158Z",
     "shell.execute_reply.started": "2022-05-27T20:27:59.554812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 是否要冻住模型的前面一些层\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        model = model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "# ResNest模型\n",
    "def resnest_model(num_classes,feature_extracting=False):\n",
    "    model=resnest50(pretrained=True)\n",
    "    set_parameter_requires_grad(model,feature_extracting)\n",
    "    num_ftrs=model.fc.in_features\n",
    "    model.fc=nn.Sequential(nn.Linear(num_ftrs,num_classes))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcfacac-0e58-4987-88d2-6d3142960178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:28:01.485839Z",
     "iopub.status.busy": "2022-05-27T20:28:01.485265Z",
     "iopub.status.idle": "2022-05-27T20:28:01.491583Z",
     "shell.execute_reply": "2022-05-27T20:28:01.490345Z",
     "shell.execute_reply.started": "2022-05-27T20:28:01.485788Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525269cb-2ed6-4ce3-ae10-17dc7d70015b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:28:02.642531Z",
     "iopub.status.busy": "2022-05-27T20:28:02.641937Z",
     "iopub.status.idle": "2022-05-27T20:28:02.652122Z",
     "shell.execute_reply": "2022-05-27T20:28:02.651225Z",
     "shell.execute_reply.started": "2022-05-27T20:28:02.642479Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_folds=5\n",
    "num_epochs=30\n",
    "learning_rate=1e-4\n",
    "wd=1e-3\n",
    "train_loss_function=CutMixCrossEntropyLoss(True)\n",
    "test_loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "results={}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "kfold=KFold(n_splits=k_folds,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a4d6e-6839-4255-98c2-0a4aa355d8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T15:47:27.252314Z",
     "iopub.status.busy": "2022-05-24T15:47:27.251385Z",
     "iopub.status.idle": "2022-05-24T15:47:27.296357Z",
     "shell.execute_reply": "2022-05-24T15:47:27.295291Z",
     "shell.execute_reply.started": "2022-05-24T15:47:27.252262Z"
    }
   },
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585103e-8bba-45aa-9a69-ebc80362d4c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T12:28:19.799250Z",
     "iopub.status.busy": "2022-05-27T12:28:19.798310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "fold=0\n",
      "********************\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [01:57<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000098\n",
      "[ Train | 001/030 ] loss = 4.10735\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 454/459 [01:50<00:01,  4.28it/s]"
     ]
    }
   ],
   "source": [
    "# Start print\n",
    "print('--------------------------------------')\n",
    "\n",
    "for fold,(train_ids,valid_ids) in enumerate(kfold.split(pd.read_csv(train_path))):\n",
    "    print(f'{fold=}')\n",
    "    print('*'*20)\n",
    "    train_subsampler=torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler=torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader=torch.utils.data.DataLoader(\n",
    "        CutMix(\n",
    "            TrainData(train_path,img_path,transform=train_transform),\n",
    "            num_class=176,beta=1.,prob=.5,num_mix=2),\n",
    "        batch_size=32,\n",
    "        sampler=train_subsampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=False)\n",
    "    valid_loader=torch.utils.data.DataLoader(\n",
    "        TrainData(train_path,img_path,transform=test_transform),\n",
    "        batch_size=32,\n",
    "        sampler=valid_subsampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=False)\n",
    "    model=resnest_model(176).to(device)\n",
    "    model.device=device\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay= wd)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1)\n",
    "    # print(optimizer.param_groups)\n",
    "    for epoch in range(num_epochs):\n",
    "        time.sleep(0.5)\n",
    "        model.train()\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            time.sleep(0.05)\n",
    "            imgs,labels=batch\n",
    "            imgs,labels=imgs.to(device),labels.to(device)\n",
    "            logits=model(imgs)\n",
    "            loss = train_loss_function(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss.append(loss.item())         \n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch+1,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_avg_loss = np.sum(train_loss) / len(train_loss)\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_avg_loss:.5f}\")\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    print('Starting validation')\n",
    "    print('saving model with loss {:.3f}'.format(train_avg_loss))\n",
    "    save_path = f'dataset/classify_leaves//model-fold-{fold}.pth'\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            imgs,labels=batch\n",
    "            imgs,labels=imgs.to(device),labels.to(device)\n",
    "            logits=model(imgs)\n",
    "            loss = test_loss_function(logits, labels)\n",
    "            valid_loss.append(loss.cpu().item())\n",
    "            valid_acc.append((logits.argmax(dim=-1)==labels).float().mean().cpu())\n",
    "        valid_avg_loss=np.sum(valid_loss)/len(valid_loss)\n",
    "        valid_avg_acc=np.sum(valid_acc)/len(valid_acc)\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{num_epochs:03d} ] loss = {valid_avg_loss:.5f}, acc = {valid_avg_acc:.5f}\")\n",
    "        print('Accuracy for fold %d: %.2f' % (fold, valid_avg_acc))\n",
    "        results[fold]=valid_avg_acc\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "total_summation=0.\n",
    "for key,value in results.items():\n",
    "    print(f'Fold {key}: {value} ')\n",
    "    total_summation+=value\n",
    "print(f'Average: {total_summation/len(results.items())} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "568b6934-69c1-4df2-99d9-8c459877f447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:49:39.009709Z",
     "iopub.status.busy": "2022-05-28T05:49:39.009118Z",
     "iopub.status.idle": "2022-05-28T05:49:39.029010Z",
     "shell.execute_reply": "2022-05-28T05:49:39.028263Z",
     "shell.execute_reply.started": "2022-05-28T05:49:39.009658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(\n",
    "                      TestData(test_path, img_path, transform = test_transform),\n",
    "                      batch_size=32, num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab7d6ee7-c53d-4ed2-95f5-c1c88b9328b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:30:52.705366Z",
     "iopub.status.busy": "2022-05-27T20:30:52.704786Z",
     "iopub.status.idle": "2022-05-27T20:36:02.246991Z",
     "shell.execute_reply": "2022-05-27T20:36:02.246194Z",
     "shell.execute_reply.started": "2022-05-27T20:30:52.705318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [01:03<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:58<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:58<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:59<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:58<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = resnest_model(176)\n",
    "model = model.to(device)\n",
    "model.device=device\n",
    "for test_fold in range(k_folds):\n",
    "    model_path = f'dataset/classify_leaves/model-fold-{test_fold}.pth'\n",
    "    saveFileName = f'dataset/classify_leaves/submission-fold-{test_fold}.csv'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(200,200))\n",
    "    preds=[]\n",
    "    for batch in tqdm(testloader):\n",
    "        imgs=batch\n",
    "        imgs=imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = tta_model(imgs)\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "    test_df=pd.read_csv(test_path)\n",
    "    test_df['label']=pd.Series(preds)\n",
    "    submission=pd.concat([test_df['image'], test_df['label']], axis=1)\n",
    "    submission.to_csv(saveFileName,index=False)\n",
    "    print(\"ResNeSt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae400c45-a173-4deb-9150-98aaa0e10ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:50:37.315741Z",
     "iopub.status.busy": "2022-05-27T20:50:37.315137Z",
     "iopub.status.idle": "2022-05-27T20:50:37.494122Z",
     "shell.execute_reply": "2022-05-27T20:50:37.493096Z",
     "shell.execute_reply.started": "2022-05-27T20:50:37.315682Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('dataset/classify_leaves/submission-fold-0.csv')\n",
    "df1 = pd.read_csv('dataset/classify_leaves/submission-fold-1.csv')\n",
    "df2 = pd.read_csv('dataset/classify_leaves/submission-fold-2.csv')\n",
    "df3 = pd.read_csv('dataset/classify_leaves/submission-fold-3.csv')\n",
    "df4 = pd.read_csv('dataset/classify_leaves/submission-fold-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9183b1a0-9cf2-4dda-ae6f-2840177265da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T20:50:39.950704Z",
     "iopub.status.busy": "2022-05-27T20:50:39.945095Z",
     "iopub.status.idle": "2022-05-27T20:50:42.917284Z",
     "shell.execute_reply": "2022-05-27T20:50:42.916566Z",
     "shell.execute_reply.started": "2022-05-27T20:50:39.950584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting results of resnest successfully saved!\n"
     ]
    }
   ],
   "source": [
    "df_res=pd.concat([df0['label'],df1['label'],df2['label'],df3['label'],df4['label']],axis=1)\n",
    "s_res=df_res.mode(axis=1)[0].apply(int)\n",
    "submission=pd.concat([df0['image'], s_res], axis=1)\n",
    "submission.columns=['image','label']\n",
    "submission.to_csv('dataset/classify_leaves/submission-resnest.csv', index=False)\n",
    "print('Voting results of resnest successfully saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db8ead-e116-435a-bfe4-7e9993572139",
   "metadata": {},
   "source": [
    "### ResNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b49a676-156c-4af9-a0fb-c2e85d4ac96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:49:49.501058Z",
     "iopub.status.busy": "2022-05-28T05:49:49.500491Z",
     "iopub.status.idle": "2022-05-28T05:49:49.510685Z",
     "shell.execute_reply": "2022-05-28T05:49:49.509699Z",
     "shell.execute_reply.started": "2022-05-28T05:49:49.501008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 是否要冻住模型的前面一些层\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        model = model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# resnext50_32x4d模型\n",
    "def resnext_model(num_classes, feature_extract = False, use_pretrained=True):\n",
    "\n",
    "    model_ft = models.resnext50_32x4d(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, num_classes))\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ebbf17b-0dd6-49c3-9fc9-e3c4050a5063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:49:49.904875Z",
     "iopub.status.busy": "2022-05-28T05:49:49.904317Z",
     "iopub.status.idle": "2022-05-28T05:49:49.914042Z",
     "shell.execute_reply": "2022-05-28T05:49:49.913052Z",
     "shell.execute_reply.started": "2022-05-28T05:49:49.904824Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "device='cuda:0'\n",
    "k_folds = 5\n",
    "num_epochs = 30\n",
    "lr = 1e-3\n",
    "wd = 1e-3\n",
    "train_loss_function = CutMixCrossEntropyLoss(True)\n",
    "test_loss_function = nn.CrossEntropyLoss()\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344a996-6b0d-40f4-b23e-58068417a413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T23:01:36.825135Z",
     "iopub.status.busy": "2022-05-27T23:01:36.824171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "FOLD 0\n",
      "********************\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.001000\n",
      "[ Train | 001/030 ] loss = 4.10203\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000976\n",
      "[ Train | 002/030 ] loss = 3.35978\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000905\n",
      "[ Train | 003/030 ] loss = 3.04898\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4个epoch的学习率：0.000794\n",
      "[ Train | 004/030 ] loss = 2.81344\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5个epoch的学习率：0.000655\n",
      "[ Train | 005/030 ] loss = 2.60863\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6个epoch的学习率：0.000500\n",
      "[ Train | 006/030 ] loss = 2.41990\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7个epoch的学习率：0.000345\n",
      "[ Train | 007/030 ] loss = 2.26612\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8个epoch的学习率：0.000206\n",
      "[ Train | 008/030 ] loss = 2.11533\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9个epoch的学习率：0.000095\n",
      "[ Train | 009/030 ] loss = 2.01173\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10个epoch的学习率：0.000024\n",
      "[ Train | 010/030 ] loss = 1.95220\n",
      "Starting epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第11个epoch的学习率：0.000000\n",
      "[ Train | 011/030 ] loss = 1.93134\n",
      "Starting epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第12个epoch的学习率：0.000024\n",
      "[ Train | 012/030 ] loss = 1.94284\n",
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第13个epoch的学习率：0.000095\n",
      "[ Train | 013/030 ] loss = 1.94857\n",
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第14个epoch的学习率：0.000206\n",
      "[ Train | 014/030 ] loss = 1.99419\n",
      "Starting epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第15个epoch的学习率：0.000345\n",
      "[ Train | 015/030 ] loss = 2.06719\n",
      "Starting epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第16个epoch的学习率：0.000500\n",
      "[ Train | 016/030 ] loss = 2.13386\n",
      "Starting epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第17个epoch的学习率：0.000655\n",
      "[ Train | 017/030 ] loss = 2.20329\n",
      "Starting epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第18个epoch的学习率：0.000794\n",
      "[ Train | 018/030 ] loss = 2.27404\n",
      "Starting epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第19个epoch的学习率：0.000905\n",
      "[ Train | 019/030 ] loss = 2.27371\n",
      "Starting epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第20个epoch的学习率：0.000976\n",
      "[ Train | 020/030 ] loss = 2.25371\n",
      "Starting epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第21个epoch的学习率：0.001000\n",
      "[ Train | 021/030 ] loss = 2.21108\n",
      "Starting epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第22个epoch的学习率：0.000976\n",
      "[ Train | 022/030 ] loss = 2.14567\n",
      "Starting epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第23个epoch的学习率：0.000905\n",
      "[ Train | 023/030 ] loss = 2.06740\n",
      "Starting epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第24个epoch的学习率：0.000794\n",
      "[ Train | 024/030 ] loss = 2.00849\n",
      "Starting epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第25个epoch的学习率：0.000655\n",
      "[ Train | 025/030 ] loss = 1.92219\n",
      "Starting epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:05<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第26个epoch的学习率：0.000500\n",
      "[ Train | 026/030 ] loss = 1.82230\n",
      "Starting epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [01:04<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第27个epoch的学习率：0.000345\n",
      "[ Train | 027/030 ] loss = 1.74374\n",
      "Starting epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 101/114 [00:57<00:07,  1.85it/s]"
     ]
    }
   ],
   "source": [
    "# Start print\n",
    "print('--------------------------------------')\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids,valid_ids) in enumerate(kfold.split(pd.read_csv(train_path))):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('*'*20)\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      CutMix(\n",
    "                          TrainData(train_path, img_path, transform = train_transform), \n",
    "                          num_class=176, beta=1.0, prob=0.5, num_mix=2),\n",
    "                      batch_size=128, sampler=train_subsampler, num_workers=4,pin_memory=False,drop_last=True)\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "                      TrainData(train_path, img_path, transform = test_transform),\n",
    "                      batch_size=128, sampler=valid_subsampler, num_workers=4,pin_memory=False,drop_last=True)\n",
    "    model = resnext_model(176)\n",
    "    model = model.to(device)\n",
    "    model.device = device\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay= wd)\n",
    "    #   optimizer = SWA(our_optimizer, swa_start=5, swa_freq =5, swa_lr=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer,T_max=10)\n",
    "    for epoch in range(0,num_epochs):\n",
    "        time.sleep(.5)\n",
    "        model.train()\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        # These are used to record information in training\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        # Iterate the training set by batches   \n",
    "        for batch in tqdm(trainloader):\n",
    "            time.sleep(0.005)\n",
    "            # Move images and labels to GPU\n",
    "            imgs, labels = batch\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward the data\n",
    "            logits = model(imgs)\n",
    "            loss = train_loss_function(logits,labels)\n",
    "            # Clear gradients in previous step\n",
    "            optimizer.zero_grad()\n",
    "            # Compute gradients for parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Compute the accuracy for current batch.\n",
    "            # acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "            # Record the loss and accuracy.\n",
    "            train_losses.append(loss.item())\n",
    "            time.sleep(0.005)\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch+1,optimizer.param_groups[0]['lr']))\n",
    "        scheduler.step()\n",
    "        train_avg_loss = np.sum(train_losses) / len(train_losses)\n",
    "        # train_acc = np.sum(train_accs) / len(train_accs)\n",
    "        # Print the information.\n",
    "        # print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_avg_loss:.5f}\")\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    print('Starting validation')\n",
    "\n",
    "    # Saving the model\n",
    "    print('saving model with loss {:.3f}'.format(train_avg_loss))\n",
    "    save_path = f'dataset/classify_leaves/model2-fold-{fold}.pth'\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    # Start Validation\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validloader):\n",
    "            time.sleep(0.005)\n",
    "            imgs, labels = batch\n",
    "            # No gradient in validation\n",
    "            logits = model(imgs.to(device))\n",
    "            loss = test_loss_function(logits,labels.to(device))\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean().cpu()\n",
    "            # Record loss and accuracy\n",
    "            valid_losses.append(loss.item())        \n",
    "            valid_accs.append(acc)\n",
    "        # The average loss and accuracy\n",
    "        valid_avg_loss = np.sum(valid_losses)/len(valid_losses)\n",
    "        valid_avg_acc = np.sum(valid_accs)/len(valid_accs)\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{num_epochs:03d} ] loss = {valid_avg_loss:.5f}, acc = {valid_avg_acc:.5f}\")\n",
    "        print('Accuracy for fold %d: %d' % (fold, valid_avg_acc))\n",
    "        print('--------------------------------------')\n",
    "        results[fold] = valid_avg_acc\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "total_summation = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} ')\n",
    "    total_summation += value\n",
    "    print(f'Average: {total_summation/len(results.items())} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b110903-efad-49c6-8f75-305d4bae17a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:06:09.526212Z",
     "iopub.status.busy": "2022-05-28T13:06:09.525723Z",
     "iopub.status.idle": "2022-05-28T13:06:09.607010Z",
     "shell.execute_reply": "2022-05-28T13:06:09.606122Z",
     "shell.execute_reply.started": "2022-05-28T13:06:09.526171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(\n",
    "                      TestData(test_path, img_path, transform = test_transform),\n",
    "                      batch_size=128, num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d138122-05a2-4871-a148-e221ece0ce3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:49:59.626288Z",
     "iopub.status.busy": "2022-05-28T05:49:59.625718Z",
     "iopub.status.idle": "2022-05-28T05:54:00.279117Z",
     "shell.execute_reply": "2022-05-28T05:54:00.277388Z",
     "shell.execute_reply.started": "2022-05-28T05:49:59.626236Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:47<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:48<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:47<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:47<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:47<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "## predict\n",
    "model = resnext_model(176)\n",
    "\n",
    "# create model and load weights from checkpoint\n",
    "model = model.to(device)\n",
    "model.device=device\n",
    "# load the all folds\n",
    "for test_fold in range(k_folds):\n",
    "    model_path = f'dataset/classify_leaves/model2-fold-{test_fold}.pth'\n",
    "    saveFileName = f'dataset/classify_leaves/submission2-fold-{test_fold}.csv'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(200,200))\n",
    "\n",
    "    # Initialize a list to store the predictions.\n",
    "    preds = []\n",
    "    # Iterate the testing set by batches.\n",
    "    for batch in tqdm(testloader):\n",
    "        imgs = batch\n",
    "        with torch.no_grad():\n",
    "            logits = tta_model(imgs.to(device))\n",
    "\n",
    "        # Take the class with greatest logit as prediction and record it.\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    test_data['label'] = pd.Series(preds)\n",
    "    submission = pd.concat([test_data['image'], test_data['label']], axis=1)\n",
    "    submission.to_csv(saveFileName, index=False)\n",
    "    print(\"ResNeXt Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a83e9902-b53b-4d3e-9374-a757129e0d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:55:23.511790Z",
     "iopub.status.busy": "2022-05-28T05:55:23.511199Z",
     "iopub.status.idle": "2022-05-28T05:55:26.643392Z",
     "shell.execute_reply": "2022-05-28T05:55:26.642586Z",
     "shell.execute_reply.started": "2022-05-28T05:55:23.511730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting results of resnest successfully saved!\n"
     ]
    }
   ],
   "source": [
    "df0 = pd.read_csv('dataset/classify_leaves/submission2-fold-0.csv')\n",
    "df1 = pd.read_csv('dataset/classify_leaves/submission2-fold-1.csv')\n",
    "df2 = pd.read_csv('dataset/classify_leaves/submission2-fold-2.csv')\n",
    "df3 = pd.read_csv('dataset/classify_leaves/submission2-fold-3.csv')\n",
    "df4 = pd.read_csv('dataset/classify_leaves/submission2-fold-4.csv')\n",
    "df_res=pd.concat([df0['label'],df1['label'],df2['label'],df3['label'],df4['label']],axis=1)\n",
    "s_res=df_res.mode(axis=1)[0].apply(int)\n",
    "submission=pd.concat([df0['image'], s_res], axis=1)\n",
    "submission.columns=['image','label']\n",
    "submission.to_csv('dataset/classify_leaves/submission2-resnext.csv', index=False)\n",
    "print('Voting results of resnest successfully saved!')\n",
    "#多众数投票到最小的那个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06921243-9617-44e4-8cf3-1a8919b68d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T05:55:32.219431Z",
     "iopub.status.busy": "2022-05-28T05:55:32.218744Z",
     "iopub.status.idle": "2022-05-28T05:55:32.228772Z",
     "shell.execute_reply": "2022-05-28T05:55:32.227765Z",
     "shell.execute_reply.started": "2022-05-28T05:55:32.219379Z"
    }
   },
   "outputs": [],
   "source": [
    "# 是否要冻住模型的前面一些层\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        model = model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# densenet161模型\n",
    "def dense_model(num_classes, feature_extract = False, use_pretrained=True):\n",
    "\n",
    "    model_ft = models.densenet161(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.classifier.in_features\n",
    "    model_ft.classifier = nn.Sequential(nn.Linear(num_ftrs, num_classes))\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91e66cd4-ba69-4048-85a9-459052666cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T06:21:28.030546Z",
     "iopub.status.busy": "2022-05-28T06:21:28.030125Z",
     "iopub.status.idle": "2022-05-28T06:21:28.037245Z",
     "shell.execute_reply": "2022-05-28T06:21:28.036453Z",
     "shell.execute_reply.started": "2022-05-28T06:21:28.030505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 5\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-3\n",
    "train_loss_function = CutMixCrossEntropyLoss(True)\n",
    "valid_loss_function = nn.CrossEntropyLoss()\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65b766-f2ba-45b1-a4a3-13f33c869466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T06:21:29.191460Z",
     "iopub.status.busy": "2022-05-28T06:21:29.191057Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "FOLD 0\n",
      "********************\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:12<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.001000\n",
      "[ Train | 001/030 ] loss = 5.12747\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:08<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000976\n",
      "[ Train | 002/030 ] loss = 4.92846\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:08<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000905\n",
      "[ Train | 003/030 ] loss = 4.71457\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:08<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4个epoch的学习率：0.000794\n",
      "[ Train | 004/030 ] loss = 4.54070\n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 457/458 [02:11<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:11<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6个epoch的学习率：0.000500\n",
      "[ Train | 006/030 ] loss = 4.27224\n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:11<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7个epoch的学习率：0.000345\n",
      "[ Train | 007/030 ] loss = 4.08584\n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:16<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8个epoch的学习率：0.000206\n",
      "[ Train | 008/030 ] loss = 3.92822\n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:16<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9个epoch的学习率：0.000095\n",
      "[ Train | 009/030 ] loss = 3.79494\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:14<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10个epoch的学习率：0.000024\n",
      "[ Train | 010/030 ] loss = 3.70301\n",
      "Starting epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:22<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第11个epoch的学习率：0.000000\n",
      "[ Train | 011/030 ] loss = 3.70056\n",
      "Starting epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:22<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第12个epoch的学习率：0.000024\n",
      "[ Train | 012/030 ] loss = 3.66800\n",
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:12<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第13个epoch的学习率：0.000095\n",
      "[ Train | 013/030 ] loss = 3.65620\n",
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:32<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第14个epoch的学习率：0.000206\n",
      "[ Train | 014/030 ] loss = 3.64286\n",
      "Starting epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:23<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第15个epoch的学习率：0.000345\n",
      "[ Train | 015/030 ] loss = 3.65930\n",
      "Starting epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:45<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第16个epoch的学习率：0.000500\n",
      "[ Train | 016/030 ] loss = 3.60034\n",
      "Starting epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:30<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第17个epoch的学习率：0.000655\n",
      "[ Train | 017/030 ] loss = 3.57605\n",
      "Starting epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:19<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第18个epoch的学习率：0.000794\n",
      "[ Train | 018/030 ] loss = 3.52599\n",
      "Starting epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:15<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第19个epoch的学习率：0.000905\n",
      "[ Train | 019/030 ] loss = 3.47018\n",
      "Starting epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:36<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第20个epoch的学习率：0.000976\n",
      "[ Train | 020/030 ] loss = 3.39725\n",
      "Starting epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:27<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第21个epoch的学习率：0.001000\n",
      "[ Train | 021/030 ] loss = 3.31806\n",
      "Starting epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [02:35<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第22个epoch的学习率：0.000976\n",
      "[ Train | 022/030 ] loss = 3.21911\n",
      "Starting epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [04:30<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第23个epoch的学习率：0.000905\n",
      "[ Train | 023/030 ] loss = 3.11915\n",
      "Starting epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [04:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第24个epoch的学习率：0.000794\n",
      "[ Train | 024/030 ] loss = 2.99069\n",
      "Starting epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [05:14<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第25个epoch的学习率：0.000655\n",
      "[ Train | 025/030 ] loss = 2.89318\n",
      "Starting epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [05:38<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第26个epoch的学习率：0.000500\n",
      "[ Train | 026/030 ] loss = 2.79258\n",
      "Starting epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [06:32<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第27个epoch的学习率：0.000345\n",
      "[ Train | 027/030 ] loss = 2.67247\n",
      "Starting epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [05:36<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第28个epoch的学习率：0.000206\n",
      "[ Train | 028/030 ] loss = 2.58164\n",
      "Starting epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [05:10<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第29个epoch的学习率：0.000095\n",
      "[ Train | 029/030 ] loss = 2.51177\n",
      "Starting epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [05:07<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第30个epoch的学习率：0.000024\n",
      "[ Train | 030/030 ] loss = 2.47977\n",
      "Training process has finished. Saving trained model.\n",
      "Starting validation\n",
      "saving model with loss 2.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:25<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 030/030 ] loss = 1.61269, acc = 0.85362\n",
      "Accuracy for fold 0: 0\n",
      "--------------------------------------\n",
      "FOLD 1\n",
      "********************\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [03:33<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.001000\n",
      "[ Train | 001/030 ] loss = 4.92309\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 292/458 [01:59<00:46,  3.60it/s]"
     ]
    }
   ],
   "source": [
    "# Start print\n",
    "print('--------------------------------------')\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids,valid_ids) in enumerate(kfold.split(pd.read_csv(train_path))):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('*'*20)\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      CutMix(\n",
    "                          TrainData(train_path, img_path, transform = train_transform), \n",
    "                          num_class=176, beta=1.0, prob=0.5, num_mix=2),\n",
    "                      batch_size=32, sampler=train_subsampler, num_workers=4,pin_memory=False,drop_last=True)\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "                      TrainData(train_path, img_path, transform = test_transform),\n",
    "                      batch_size=32, sampler=valid_subsampler, num_workers=4,pin_memory=False,drop_last=True)\n",
    "    model = dense_model(176)\n",
    "    model = model.to(device)\n",
    "    model.device = device\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay= wd)\n",
    "    #   optimizer = SWA(our_optimizer, swa_start=5, swa_freq =5, swa_lr=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer,T_max=10)\n",
    "    for epoch in range(0,num_epochs):\n",
    "        time.sleep(.5)\n",
    "        model.train()\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        # These are used to record information in training\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        # Iterate the training set by batches   \n",
    "        for batch in tqdm(trainloader):\n",
    "            time.sleep(.005)\n",
    "            # Move images and labels to GPU\n",
    "            imgs, labels = batch\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward the data\n",
    "            logits = model(imgs)\n",
    "            loss = train_loss_function(logits,labels)\n",
    "            # Clear gradients in previous step\n",
    "            optimizer.zero_grad()\n",
    "            # Compute gradients for parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Compute the accuracy for current batch.\n",
    "            # acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "            # Record the loss and accuracy.\n",
    "            train_losses.append(loss.item())\n",
    "            time.sleep(.005)\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch+1,optimizer.param_groups[0]['lr']))\n",
    "        scheduler.step()\n",
    "        train_avg_loss = np.sum(train_losses) / len(train_losses)\n",
    "        # train_acc = np.sum(train_accs) / len(train_accs)\n",
    "        # Print the information.\n",
    "        # print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{num_epochs:03d} ] loss = {train_avg_loss:.5f}\")\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    print('Starting validation')\n",
    "\n",
    "    # Saving the model\n",
    "    print('saving model with loss {:.3f}'.format(train_avg_loss))\n",
    "    save_path = f'dataset/classify_leaves/model3-fold-{fold}.pth'\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    # Start Validation\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validloader):\n",
    "            time.sleep(.005)\n",
    "            imgs, labels = batch\n",
    "            # No gradient in validation\n",
    "            logits = model(imgs.to(device))\n",
    "            loss = valid_loss_function(logits,labels.to(device))\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean().cpu()\n",
    "            # Record loss and accuracy\n",
    "            valid_losses.append(loss.item())        \n",
    "            valid_accs.append(acc)\n",
    "            time.sleep(.005)\n",
    "        # The average loss and accuracy\n",
    "        valid_avg_loss = np.sum(valid_losses)/len(valid_losses)\n",
    "        valid_avg_acc = np.sum(valid_accs)/len(valid_accs)\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{num_epochs:03d} ] loss = {valid_avg_loss:.5f}, acc = {valid_avg_acc:.5f}\")\n",
    "        print('Accuracy for fold %d: %d' % (fold, valid_avg_acc))\n",
    "        print('--------------------------------------')\n",
    "        results[fold] = valid_avg_acc\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "total_summation = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} ')\n",
    "    total_summation += value\n",
    "    print(f'Average: {total_summation/len(results.items())} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae7ab4ea-3dd4-4e59-afd6-1a312d49db72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:06:23.180635Z",
     "iopub.status.busy": "2022-05-28T13:06:23.180020Z",
     "iopub.status.idle": "2022-05-28T13:06:23.206748Z",
     "shell.execute_reply": "2022-05-28T13:06:23.205879Z",
     "shell.execute_reply.started": "2022-05-28T13:06:23.180583Z"
    }
   },
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(\n",
    "                      TestData(test_path, img_path, transform = test_transform),\n",
    "                      batch_size=128, num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bd2484a-6433-4ba8-a60c-e4c51119983f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:06:26.053015Z",
     "iopub.status.busy": "2022-05-28T13:06:26.052404Z",
     "iopub.status.idle": "2022-05-28T13:14:05.742934Z",
     "shell.execute_reply": "2022-05-28T13:14:05.741950Z",
     "shell.execute_reply.started": "2022-05-28T13:06:26.052962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:25<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:26<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:26<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:26<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [01:26<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "model = dense_model(176)\n",
    "\n",
    "# create model and load weights from checkpoint\n",
    "model = model.to(device)\n",
    "model.device=device\n",
    "# load the all folds\n",
    "for test_fold in range(k_folds):\n",
    "    model_path = f'dataset/classify_leaves/model3-fold-{test_fold}.pth'\n",
    "    saveFileName = f'dataset/classify_leaves/submission3-fold-{test_fold}.csv'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(200,200))\n",
    "\n",
    "    # Initialize a list to store the predictions.\n",
    "    preds = []\n",
    "    # Iterate the testing set by batches.\n",
    "    for batch in tqdm(testloader):\n",
    "        imgs = batch\n",
    "        with torch.no_grad():\n",
    "            logits = tta_model(imgs.to(device))\n",
    "\n",
    "        # Take the class with greatest logit as prediction and record it.\n",
    "        preds.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    test_data['label'] = pd.Series(preds)\n",
    "    submission = pd.concat([test_data['image'], test_data['label']], axis=1)\n",
    "    submission.to_csv(saveFileName, index=False)\n",
    "    print(\"Dense Model Results Done!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be3f73b2-0a17-4dce-ba67-6e9e670cb10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:17:26.361180Z",
     "iopub.status.busy": "2022-05-28T13:17:26.360420Z",
     "iopub.status.idle": "2022-05-28T13:17:29.735274Z",
     "shell.execute_reply": "2022-05-28T13:17:29.734653Z",
     "shell.execute_reply.started": "2022-05-28T13:17:26.361101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting results of resnest successfully saved!\n"
     ]
    }
   ],
   "source": [
    "df0 = pd.read_csv('dataset/classify_leaves/submission3-fold-0.csv')\n",
    "df1 = pd.read_csv('dataset/classify_leaves/submission3-fold-1.csv')\n",
    "df2 = pd.read_csv('dataset/classify_leaves/submission3-fold-2.csv')\n",
    "df3 = pd.read_csv('dataset/classify_leaves/submission3-fold-3.csv')\n",
    "df4 = pd.read_csv('dataset/classify_leaves/submission3-fold-4.csv')\n",
    "df_res=pd.concat([df0['label'],df1['label'],df2['label'],df3['label'],df4['label']],axis=1)\n",
    "s_res=df_res.mode(axis=1)[0].apply(int)\n",
    "submission=pd.concat([df0['image'], s_res], axis=1)\n",
    "submission.columns=['image','label']\n",
    "submission.to_csv('dataset/classify_leaves/submission3-densenet.csv', index=False)\n",
    "print('Voting results of resnest successfully saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c91112c-236d-4b05-b526-060f880ea498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T13:23:52.534572Z",
     "iopub.status.busy": "2022-05-28T13:23:52.533896Z",
     "iopub.status.idle": "2022-05-28T13:23:52.643629Z",
     "shell.execute_reply": "2022-05-28T13:23:52.642945Z",
     "shell.execute_reply.started": "2022-05-28T13:23:52.534517Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_resnest = pd.read_csv('dataset/classify_leaves/submission-resnest.csv')\n",
    "df_resnext = pd.read_csv('dataset/classify_leaves/submission2-resnext.csv')\n",
    "df_densenet = pd.read_csv('dataset/classify_leaves/submission3-densenet.csv')\n",
    "df_res=pd.concat([df_resnest['label'],df_resnext['label'],df_densenet['label']],axis=1)\n",
    "np_res=np.array(df_res.values)\n",
    "res=[]\n",
    "for i in range(len(df_res)):\n",
    "    tmp=np_res[i]\n",
    "    # print(tmp)\n",
    "    tmp_m=collections.Counter(tmp)\n",
    "    li=sorted([*zip(tmp_m.keys(),tmp_m.values())],key=lambda x:-x[1])\n",
    "    # print(li)\n",
    "    if len(li)==1 or li[1][1]<li[0][1]:\n",
    "        res.append(li[0][0])\n",
    "    else:\n",
    "        res.append(tmp[0])\n",
    "final_res=pd.Series([num_to_cls[i] for i in res],name='label')\n",
    "final_df=pd.concat([df_resnest['image'], final_res], axis=1)\n",
    "final_df.to_csv('dataset/classify_leaves/final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57c529cd-7949-4964-84fc-5b801391e348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T15:35:53.468480Z",
     "iopub.status.busy": "2022-05-26T15:35:53.467824Z",
     "iopub.status.idle": "2022-05-26T15:35:53.964807Z",
     "shell.execute_reply": "2022-05-26T15:35:53.963850Z",
     "shell.execute_reply.started": "2022-05-26T15:35:53.468397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('dataset/classify_leaves/submission-fold-0.csv')\n",
    "df1 = pd.read_csv('dataset/classify_leaves/submission-fold-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4895370a-1e2a-42a9-98b0-2bcd2864688a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T15:35:56.869122Z",
     "iopub.status.busy": "2022-05-26T15:35:56.868727Z",
     "iopub.status.idle": "2022-05-26T15:35:57.006298Z",
     "shell.execute_reply": "2022-05-26T15:35:57.005645Z",
     "shell.execute_reply.started": "2022-05-26T15:35:56.869089Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/18353.jpg</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/18354.jpg</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/18355.jpg</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/18356.jpg</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/18357.jpg</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>images/27148.jpg</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>images/27149.jpg</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8797</th>\n",
       "      <td>images/27150.jpg</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>images/27151.jpg</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>images/27152.jpg</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image  label\n",
       "0     images/18353.jpg     22\n",
       "1     images/18354.jpg    123\n",
       "2     images/18355.jpg    150\n",
       "3     images/18356.jpg    110\n",
       "4     images/18357.jpg    121\n",
       "...                ...    ...\n",
       "8795  images/27148.jpg    112\n",
       "8796  images/27149.jpg    162\n",
       "8797  images/27150.jpg    112\n",
       "8798  images/27151.jpg     78\n",
       "8799  images/27152.jpg     73\n",
       "\n",
       "[8800 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6942e351-05d7-47d4-9e99-99446ee7e63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T15:20:40.300838Z",
     "iopub.status.busy": "2022-05-26T15:20:40.300411Z",
     "iopub.status.idle": "2022-05-26T15:20:40.304889Z",
     "shell.execute_reply": "2022-05-26T15:20:40.304206Z",
     "shell.execute_reply.started": "2022-05-26T15:20:40.300804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a0098f-53a1-4a49-9b9a-3f4b1684c334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T15:21:22.272978Z",
     "iopub.status.busy": "2022-05-26T15:21:22.272374Z",
     "iopub.status.idle": "2022-05-26T15:21:22.281149Z",
     "shell.execute_reply": "2022-05-26T15:21:22.280204Z",
     "shell.execute_reply.started": "2022-05-26T15:21:22.272925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faa233-32c0-4bbf-856c-41fad1da386b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
