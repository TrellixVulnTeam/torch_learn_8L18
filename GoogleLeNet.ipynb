{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fecf81-f491-4a0e-a4ed-b30d51224ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as Data\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb1d1dc6-3d44-4ed3-a7b5-e1aef3fa9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        return torch.cat([p1, p2, p3, p4], 1)  #输出通道维度为1，连接4个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten())\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_train = datasets.FashionMNIST(root='dataset/Fashion_Minist', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.FashionMNIST(root='dataset/Fashion_Minist', train=False, transform=transform, download=True)\n",
    "train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = [0, 0]\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X, list):\n",
    "            X = [x.to(device) for x in X]\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            metric[0] += torch.sum(torch.argmax(net(X), dim=1) == y).item()\n",
    "            metric[1] += y.shape[0]\n",
    "        return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    num_batches = len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        metric = [0, 0, 0]\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            metric[0] += l * X.shape[0]\n",
    "            metric[1] += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "            metric[2] += y.shape[0]\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                print('epoch %d, batch %d, loss %.4f, train acc %.3f' % (epoch + 1, i + 1, train_l, train_acc))\n",
    "        n += y.shape[0]\n",
    "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    print(f'loss {train_l:.3f},train acc {train_acc:.3f},test acc {test_acc:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, batch 93, loss 2.3029, train acc 0.103\n",
      "epoch 1, batch 186, loss 2.3017, train acc 0.108\n",
      "epoch 1, batch 279, loss 2.2907, train acc 0.124\n",
      "epoch 1, batch 372, loss 2.2496, train acc 0.156\n",
      "epoch 1, batch 465, loss 2.2637, train acc 0.155\n",
      "epoch 1, batch 469, loss 2.2640, train acc 0.155\n",
      "epoch 2, batch 93, loss 2.2804, train acc 0.146\n",
      "epoch 2, batch 186, loss 2.0051, train acc 0.250\n",
      "epoch 2, batch 279, loss 1.7644, train acc 0.329\n",
      "epoch 2, batch 372, loss 1.5987, train acc 0.383\n",
      "epoch 2, batch 465, loss 1.4761, train acc 0.424\n",
      "epoch 2, batch 469, loss 1.4719, train acc 0.426\n",
      "epoch 3, batch 93, loss 0.9370, train acc 0.610\n",
      "epoch 3, batch 186, loss 0.9416, train acc 0.618\n",
      "epoch 3, batch 279, loss 0.9115, train acc 0.632\n",
      "epoch 3, batch 372, loss 0.8769, train acc 0.649\n",
      "epoch 3, batch 465, loss 0.8375, train acc 0.668\n",
      "epoch 3, batch 469, loss 0.8355, train acc 0.669\n",
      "epoch 4, batch 93, loss 0.6337, train acc 0.766\n",
      "epoch 4, batch 186, loss 0.6122, train acc 0.770\n",
      "epoch 4, batch 279, loss 0.5930, train acc 0.778\n",
      "epoch 4, batch 372, loss 0.5736, train acc 0.785\n",
      "epoch 4, batch 465, loss 0.5596, train acc 0.790\n",
      "epoch 4, batch 469, loss 0.5589, train acc 0.790\n",
      "epoch 5, batch 93, loss 0.7038, train acc 0.740\n",
      "epoch 5, batch 186, loss 0.6063, train acc 0.776\n",
      "epoch 5, batch 279, loss 0.5618, train acc 0.792\n",
      "epoch 5, batch 372, loss 0.5347, train acc 0.801\n",
      "epoch 5, batch 465, loss 0.5146, train acc 0.809\n",
      "epoch 5, batch 469, loss 0.5136, train acc 0.809\n",
      "epoch 6, batch 93, loss 0.4568, train acc 0.830\n",
      "epoch 6, batch 186, loss 0.4348, train acc 0.839\n",
      "epoch 6, batch 279, loss 0.4268, train acc 0.841\n",
      "epoch 6, batch 372, loss 0.4195, train acc 0.843\n",
      "epoch 6, batch 465, loss 0.4132, train acc 0.845\n",
      "epoch 6, batch 469, loss 0.4126, train acc 0.845\n",
      "epoch 7, batch 93, loss 0.3865, train acc 0.854\n",
      "epoch 7, batch 186, loss 0.3808, train acc 0.856\n",
      "epoch 7, batch 279, loss 0.3771, train acc 0.857\n",
      "epoch 7, batch 372, loss 0.3707, train acc 0.861\n",
      "epoch 7, batch 465, loss 0.3666, train acc 0.861\n",
      "epoch 7, batch 469, loss 0.3663, train acc 0.861\n",
      "epoch 8, batch 93, loss 0.3401, train acc 0.874\n",
      "epoch 8, batch 186, loss 0.3428, train acc 0.872\n",
      "epoch 8, batch 279, loss 0.3401, train acc 0.872\n",
      "epoch 8, batch 372, loss 0.3400, train acc 0.872\n",
      "epoch 8, batch 465, loss 0.3393, train acc 0.872\n",
      "epoch 8, batch 469, loss 0.3390, train acc 0.873\n",
      "epoch 9, batch 93, loss 0.3146, train acc 0.881\n",
      "epoch 9, batch 186, loss 0.3155, train acc 0.882\n",
      "epoch 9, batch 279, loss 0.3157, train acc 0.882\n",
      "epoch 9, batch 372, loss 0.3143, train acc 0.881\n",
      "epoch 9, batch 465, loss 0.3137, train acc 0.882\n",
      "epoch 9, batch 469, loss 0.3137, train acc 0.882\n",
      "epoch 10, batch 93, loss 0.3074, train acc 0.884\n",
      "epoch 10, batch 186, loss 0.3025, train acc 0.887\n",
      "epoch 10, batch 279, loss 0.3004, train acc 0.887\n",
      "epoch 10, batch 372, loss 0.2979, train acc 0.888\n",
      "epoch 10, batch 465, loss 0.2950, train acc 0.888\n",
      "epoch 10, batch 469, loss 0.2946, train acc 0.888\n",
      "loss 0.295,train acc 0.888,test acc 0.867\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr, 'cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}