{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36943e01-434a-4852-99c1-56e5aef9c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data as Data\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea65db82-241e-4a3a-abf0-39a0f9392561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # momentum 一般选0.9或0.1\n",
    "    if not torch.is_grad_enabled():  #做推理的时候用全局的均值和方差，因为可能就一个样本\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)  #2维为全连接层，4维为卷积层\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)  #每一个通道的均值\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean  #用当前批量的均值更新全局的均值\n",
    "        moving_var = momentum * moving_var + (1 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  #更新gamma和beta\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                    nn.Linear(16 * 4 * 4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "                    nn.Linear(84, 10))\n",
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 1., 10, 256\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_train = datasets.FashionMNIST(root='dataset/Fashion_Minist', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.FashionMNIST(root='dataset/Fashion_Minist', train=False, transform=transform, download=True)\n",
    "train_iter = Data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_iter = Data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = [0, 0]\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X, list):\n",
    "            X = [x.to(device) for x in X]\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            metric[0] += torch.sum(torch.argmax(net(X), dim=1) == y).item()\n",
    "            metric[1] += y.shape[0]\n",
    "        return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    num_batches = len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        metric = [0, 0, 0]\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            metric[0] += l * X.shape[0]\n",
    "            metric[1] += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "            metric[2] += y.shape[0]\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                print('epoch %d, batch %d, loss %.4f, train acc %.3f' % (epoch + 1, i + 1, train_l, train_acc))\n",
    "        n += y.shape[0]\n",
    "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    print(f'loss {train_l:.3f},train acc {train_acc:.3f},test acc {test_acc:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, batch 47, loss 1.1564, train acc 0.605\n",
      "epoch 1, batch 94, loss 0.9509, train acc 0.668\n",
      "epoch 1, batch 141, loss 0.8486, train acc 0.700\n",
      "epoch 1, batch 188, loss 0.7799, train acc 0.722\n",
      "epoch 1, batch 235, loss 0.7310, train acc 0.738\n",
      "epoch 2, batch 47, loss 0.5130, train acc 0.810\n",
      "epoch 2, batch 94, loss 0.5019, train acc 0.814\n",
      "epoch 2, batch 141, loss 0.4826, train acc 0.823\n",
      "epoch 2, batch 188, loss 0.4740, train acc 0.826\n",
      "epoch 2, batch 235, loss 0.4673, train acc 0.829\n",
      "epoch 3, batch 47, loss 0.4139, train acc 0.850\n",
      "epoch 3, batch 94, loss 0.4148, train acc 0.848\n",
      "epoch 3, batch 141, loss 0.4019, train acc 0.854\n",
      "epoch 3, batch 188, loss 0.3977, train acc 0.855\n",
      "epoch 3, batch 235, loss 0.3928, train acc 0.857\n",
      "epoch 4, batch 47, loss 0.3545, train acc 0.867\n",
      "epoch 4, batch 94, loss 0.3526, train acc 0.871\n",
      "epoch 4, batch 141, loss 0.3537, train acc 0.871\n",
      "epoch 4, batch 188, loss 0.3533, train acc 0.871\n",
      "epoch 4, batch 235, loss 0.3517, train acc 0.872\n",
      "epoch 5, batch 47, loss 0.3303, train acc 0.880\n",
      "epoch 5, batch 94, loss 0.3277, train acc 0.880\n",
      "epoch 5, batch 141, loss 0.3291, train acc 0.878\n",
      "epoch 5, batch 188, loss 0.3302, train acc 0.878\n",
      "epoch 5, batch 235, loss 0.3297, train acc 0.878\n",
      "epoch 6, batch 47, loss 0.3211, train acc 0.879\n",
      "epoch 6, batch 94, loss 0.3194, train acc 0.881\n",
      "epoch 6, batch 141, loss 0.3114, train acc 0.885\n",
      "epoch 6, batch 188, loss 0.3105, train acc 0.886\n",
      "epoch 6, batch 235, loss 0.3087, train acc 0.886\n",
      "epoch 7, batch 47, loss 0.2995, train acc 0.889\n",
      "epoch 7, batch 94, loss 0.2955, train acc 0.890\n",
      "epoch 7, batch 141, loss 0.2940, train acc 0.892\n",
      "epoch 7, batch 188, loss 0.2920, train acc 0.892\n",
      "epoch 7, batch 235, loss 0.2938, train acc 0.892\n",
      "epoch 8, batch 47, loss 0.2711, train acc 0.898\n",
      "epoch 8, batch 94, loss 0.2737, train acc 0.897\n",
      "epoch 8, batch 141, loss 0.2777, train acc 0.896\n",
      "epoch 8, batch 188, loss 0.2800, train acc 0.895\n",
      "epoch 8, batch 235, loss 0.2820, train acc 0.895\n",
      "epoch 9, batch 47, loss 0.2592, train acc 0.904\n",
      "epoch 9, batch 94, loss 0.2691, train acc 0.902\n",
      "epoch 9, batch 141, loss 0.2696, train acc 0.901\n",
      "epoch 9, batch 188, loss 0.2701, train acc 0.901\n",
      "epoch 9, batch 235, loss 0.2696, train acc 0.900\n",
      "epoch 10, batch 47, loss 0.2654, train acc 0.902\n",
      "epoch 10, batch 94, loss 0.2696, train acc 0.900\n",
      "epoch 10, batch 141, loss 0.2646, train acc 0.902\n",
      "epoch 10, batch 188, loss 0.2632, train acc 0.903\n",
      "epoch 10, batch 235, loss 0.2612, train acc 0.904\n",
      "loss 0.261,train acc 0.904,test acc 0.816\n"
     ]
    }
   ],
   "source": [
    "train_ch6(net2, train_iter, test_iter, num_epochs, lr, 'cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([3.4200, 2.5762, 2.6140, 2.8960, 2.0903, 3.6441], device='cuda:0',\n        grad_fn=<ReshapeAliasBackward0>),\n tensor([-1.9752,  2.7767,  2.8723,  0.6223, -0.4964, -2.4448], device='cuda:0',\n        grad_fn=<ReshapeAliasBackward0>))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2[1].gamma.reshape((-1,)), net2[1].beta.reshape((-1,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#调包实现\n",
    "net3 = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Flatten(), nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(), nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "                    nn.Linear(84, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, batch 47, loss 1.0964, train acc 0.621\n",
      "epoch 1, batch 94, loss 0.8633, train acc 0.697\n",
      "epoch 1, batch 141, loss 0.7476, train acc 0.736\n",
      "epoch 1, batch 188, loss 0.6780, train acc 0.760\n",
      "epoch 1, batch 235, loss 0.6325, train acc 0.776\n",
      "epoch 2, batch 47, loss 0.4119, train acc 0.852\n",
      "epoch 2, batch 94, loss 0.4054, train acc 0.854\n",
      "epoch 2, batch 141, loss 0.3952, train acc 0.858\n",
      "epoch 2, batch 188, loss 0.3943, train acc 0.858\n",
      "epoch 2, batch 235, loss 0.3909, train acc 0.859\n",
      "epoch 3, batch 47, loss 0.3439, train acc 0.876\n",
      "epoch 3, batch 94, loss 0.3557, train acc 0.872\n",
      "epoch 3, batch 141, loss 0.3483, train acc 0.874\n",
      "epoch 3, batch 188, loss 0.3439, train acc 0.876\n",
      "epoch 3, batch 235, loss 0.3426, train acc 0.876\n",
      "epoch 4, batch 47, loss 0.3398, train acc 0.877\n",
      "epoch 4, batch 94, loss 0.3355, train acc 0.879\n",
      "epoch 4, batch 141, loss 0.3285, train acc 0.882\n",
      "epoch 4, batch 188, loss 0.3272, train acc 0.882\n",
      "epoch 4, batch 235, loss 0.3229, train acc 0.884\n",
      "epoch 5, batch 47, loss 0.2979, train acc 0.891\n",
      "epoch 5, batch 94, loss 0.3016, train acc 0.890\n",
      "epoch 5, batch 141, loss 0.2999, train acc 0.891\n",
      "epoch 5, batch 188, loss 0.3027, train acc 0.889\n",
      "epoch 5, batch 235, loss 0.3046, train acc 0.888\n",
      "epoch 6, batch 47, loss 0.2920, train acc 0.895\n",
      "epoch 6, batch 94, loss 0.2872, train acc 0.894\n",
      "epoch 6, batch 141, loss 0.2894, train acc 0.893\n",
      "epoch 6, batch 188, loss 0.2904, train acc 0.894\n",
      "epoch 6, batch 235, loss 0.2892, train acc 0.894\n",
      "epoch 7, batch 47, loss 0.2673, train acc 0.903\n",
      "epoch 7, batch 94, loss 0.2749, train acc 0.899\n",
      "epoch 7, batch 141, loss 0.2796, train acc 0.897\n",
      "epoch 7, batch 188, loss 0.2741, train acc 0.899\n",
      "epoch 7, batch 235, loss 0.2759, train acc 0.898\n",
      "epoch 8, batch 47, loss 0.2688, train acc 0.899\n",
      "epoch 8, batch 94, loss 0.2632, train acc 0.903\n",
      "epoch 8, batch 141, loss 0.2644, train acc 0.902\n",
      "epoch 8, batch 188, loss 0.2653, train acc 0.902\n",
      "epoch 8, batch 235, loss 0.2663, train acc 0.902\n",
      "epoch 9, batch 47, loss 0.2508, train acc 0.907\n",
      "epoch 9, batch 94, loss 0.2567, train acc 0.906\n",
      "epoch 9, batch 141, loss 0.2551, train acc 0.906\n",
      "epoch 9, batch 188, loss 0.2565, train acc 0.906\n",
      "epoch 9, batch 235, loss 0.2591, train acc 0.905\n",
      "epoch 10, batch 47, loss 0.2464, train acc 0.908\n",
      "epoch 10, batch 94, loss 0.2508, train acc 0.908\n",
      "epoch 10, batch 141, loss 0.2474, train acc 0.909\n",
      "epoch 10, batch 188, loss 0.2478, train acc 0.908\n",
      "epoch 10, batch 235, loss 0.2490, train acc 0.908\n",
      "loss 0.249,train acc 0.908,test acc 0.887\n"
     ]
    }
   ],
   "source": [
    "train_ch6(net3, train_iter, test_iter, num_epochs, lr, 'cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}